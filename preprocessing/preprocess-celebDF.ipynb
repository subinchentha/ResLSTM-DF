{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM6sF7vr3qb2WfkuXWL05UB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","!pip install opencv-python\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import multiprocessing\n","from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n","import torch\n","import gc\n","import time\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define paths based on your Google Drive directory structure\n","ROOT_DIR = \"/content/drive/MyDrive/Celeb-DF\"\n","REAL_DIRS = [\n","    os.path.join(ROOT_DIR, \"YouTube-real\"),\n","    os.path.join(ROOT_DIR, \"Celeb-real\"),\n","    os.path.join(ROOT_DIR, \"Actors-real\"),\n","    os.path.join(ROOT_DIR, \"YouTube-real-FF\")\n","]\n","FAKE_DIRS = [os.path.join(ROOT_DIR, \"Celeb-synthesis\")]\n","OUTPUT_DIR = os.path.join(ROOT_DIR, \"processed_data\")\n","FACE_DIR = os.path.join(OUTPUT_DIR, \"face_clips\")\n","METADATA_DIR = os.path.join(OUTPUT_DIR, \"metadata\")\n","\n","# Create necessary directories\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(FACE_DIR, exist_ok=True)\n","os.makedirs(METADATA_DIR, exist_ok=True)\n","os.makedirs(os.path.join(FACE_DIR, \"real\"), exist_ok=True)\n","os.makedirs(os.path.join(FACE_DIR, \"fake\"), exist_ok=True)\n"],"metadata":{"id":"NM-_CxS8gRNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if GPU is available\n","USE_GPU = torch.cuda.is_available()\n","print(f\"GPU available: {USE_GPU}\")\n","if USE_GPU:\n","    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","\n","# Determine number of CPU cores for parallel processing\n","# For video processing, using slightly fewer than max cores can avoid overloading\n","NUM_CORES = max(1, multiprocessing.cpu_count() - 1)\n","print(f\"Using {NUM_CORES} CPU cores for processing\")\n","\n","# Performance optimization settings\n","FRAMES_PER_VIDEO = 15  # Reduced from 30\n","TARGET_FACE_SIZE = (128, 128)  # Reduced from 224x224\n","MIN_FACE_FRAMES = 5  # Reduced from 10\n","BATCH_SIZE = 8 if USE_GPU else 4  # Increase batch size for GPU processing\n","MAX_VIDEOS_PER_RUN = 500  # Process in chunks to avoid memory issues # Number of videos to process per worker\n","\n","# Pre-load the face detector\n","face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n","face_cascade = cv2.CascadeClassifier(face_cascade_path)"],"metadata":{"id":"2wpuz0UhgRP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n","    \"\"\"\n","    Extract evenly spaced frames from a video - optimized version\n","    \"\"\"\n","    try:\n","        cap = cv2.VideoCapture(video_path)\n","\n","        if not cap.isOpened():\n","            return []\n","\n","        # Get video properties\n","        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        if frame_count <= 0:\n","            return []\n","\n","        # Calculate frame indices to extract\n","        indices = np.linspace(0, frame_count-1, num_frames, dtype=int)\n","\n","        frames = []\n","        for i in indices:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","            ret, frame = cap.read()\n","            if ret and frame is not None and frame.size > 0:\n","                # Downsize frame immediately to speed up face detection\n","                frame = cv2.resize(frame, (320, 240))\n","                frames.append(frame)\n","\n","        cap.release()\n","        return frames\n","    except Exception as e:\n","        return []"],"metadata":{"id":"SI0p1INDgRTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_face_with_opencv(frame, target_size=TARGET_FACE_SIZE):\n","    \"\"\"\n","    Extract face using only OpenCV for speed\n","    \"\"\"\n","    try:\n","        # Convert to grayscale for face detection\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Detect faces with relaxed parameters for speed\n","        faces = face_cascade.detectMultiScale(\n","            gray,\n","            scaleFactor=1.2,  # Increased for speed\n","            minNeighbors=3,   # Reduced for speed\n","            minSize=(20, 20), # Reduced minimum face size\n","            flags=cv2.CASCADE_SCALE_IMAGE\n","        )\n","\n","        if len(faces) == 0:\n","            return None\n","\n","        # Use largest face if multiple faces detected\n","        if len(faces) > 1:\n","            faces = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)\n","\n","        x, y, w, h = faces[0]\n","\n","        # Add smaller margin\n","        margin = min(w, h) // 8\n","        height, width = frame.shape[:2]\n","\n","        # Ensure bounds\n","        y = max(0, y - margin)\n","        y2 = min(height, y + h + margin)\n","        x = max(0, x - margin)\n","        x2 = min(width, x + w + margin)\n","\n","        # Extract and resize face\n","        face_img = frame[y:y2, x:x2]\n","        return cv2.resize(face_img, target_size)\n","    except Exception as e:\n","        return None\n"],"metadata":{"id":"ramU9T8agRWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_video(video_path, output_dir, label, video_id=None):\n","    \"\"\"\n","    Process a video: extract faces and save as a face clip - optimized version\n","    \"\"\"\n","    try:\n","        if video_id is None:\n","            video_id = os.path.splitext(os.path.basename(video_path))[0]\n","\n","        output_path = os.path.join(output_dir, f\"{video_id}.npy\")\n","\n","        # Skip if already processed\n","        if os.path.exists(output_path):\n","            return output_path\n","\n","        # Extract frames\n","        frames = extract_frames(video_path)\n","        if not frames:\n","            return None\n","\n","        # Extract faces using only OpenCV (faster)\n","        face_frames = []\n","        for frame in frames:\n","            face = extract_face_with_opencv(frame)\n","            if face is not None:\n","                face_frames.append(face)\n","\n","        # Require minimum number of face frames (reduced threshold)\n","        if len(face_frames) < MIN_FACE_FRAMES:\n","            return None\n","\n","        # Pad or truncate to exactly FRAMES_PER_VIDEO frames\n","        if len(face_frames) < FRAMES_PER_VIDEO:\n","            # Duplicate last frame if needed\n","            face_frames.extend([face_frames[-1]] * (FRAMES_PER_VIDEO - len(face_frames)))\n","        else:\n","            face_frames = face_frames[:FRAMES_PER_VIDEO]\n","\n","        # Convert to numpy array and save\n","        face_frames = np.array(face_frames)\n","        np.save(output_path, face_frames)\n","\n","        return output_path\n","    except Exception as e:\n","        return None"],"metadata":{"id":"ZVaLPQxegRZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_video_batch(batch_videos, output_dir, label):\n","    \"\"\"Process a batch of videos sequentially\"\"\"\n","    results = []\n","    for video_path in batch_videos:\n","        result = process_video(video_path, output_dir, label)\n","        if result:\n","            results.append(result)\n","    return results"],"metadata":{"id":"LHznyn0dgRbz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_video_files(dirs):\n","    \"\"\"Get all video files from directories\"\"\"\n","    video_files = []\n","    for dir_path in dirs:\n","        if not os.path.exists(dir_path):\n","            print(f\"Warning: Directory does not exist: {dir_path}\")\n","            continue\n","\n","        for root, _, files in os.walk(dir_path):\n","            for file in files:\n","                if file.lower().endswith(('.mp4', '.avi', '.mov')):\n","                    video_path = os.path.join(root, file)\n","                    video_files.append(video_path)\n","    return video_files"],"metadata":{"id":"UZ3vFk9XgRd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def downsample_videos(video_paths, target_count=None, ratio=None, seed=42):\n","    \"\"\"\n","    Downsample a list of video paths to achieve target count or ratio.\n","\n","    Args:\n","        video_paths: List of video paths\n","        target_count: Target number of videos (used if not None)\n","        ratio: Target ratio compared to real videos (used if target_count is None)\n","        seed: Random seed for reproducibility\n","\n","    Returns:\n","        Downsampled list of video paths\n","    \"\"\"\n","    if not video_paths:\n","        return []\n","\n","    if target_count is None and ratio is None:\n","        return video_paths\n","\n","    # If target count is specified, use it\n","    if target_count is not None:\n","        if target_count >= len(video_paths):\n","            return video_paths\n","\n","        # Random sampling without replacement\n","        np.random.seed(seed)\n","        indices = np.random.choice(len(video_paths), target_count, replace=False)\n","        return [video_paths[i] for i in indices]\n","\n","    # If ratio is specified, use it\n","    if ratio is not None:\n","        target_count = int(len(video_paths) * ratio)\n","        return downsample_videos(video_paths, target_count=target_count, seed=seed)"],"metadata":{"id":"cPoq_ZhvgRgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_videos_in_parallel(video_paths, output_dir, label):\n","    \"\"\"Process videos in parallel using batches\"\"\"\n","    results = []\n","\n","    # Create batches\n","    batches = []\n","    for i in range(0, len(video_paths), BATCH_SIZE):\n","        batches.append(video_paths[i:i+BATCH_SIZE])\n","\n","    # Process batches in parallel\n","    with ProcessPoolExecutor(max_workers=NUM_CORES) as executor:\n","        futures = [executor.submit(process_video_batch, batch, output_dir, label)\n","                  for batch in batches]\n","\n","        # Track progress with better reporting\n","        completed = 0\n","        start_time = time.time()\n","        total_batches = len(batches)\n","\n","        for future in as_completed(futures):\n","            batch_results = future.result()\n","            results.extend(batch_results)\n","\n","            # Update progress\n","            completed += 1\n","            elapsed = time.time() - start_time\n","            videos_per_hour = (completed * BATCH_SIZE) / (elapsed / 3600)\n","\n","            # Estimate remaining time\n","            remaining_batches = total_batches - completed\n","            if completed > 0:\n","                eta_seconds = (elapsed / completed) * remaining_batches\n","                eta_hours = eta_seconds / 3600\n","                print(f\"\\rProcessed {completed}/{total_batches} batches \"\n","                      f\"({len(results)} successful) | \"\n","                      f\"Speed: {videos_per_hour:.2f} videos/hour | \"\n","                      f\"ETA: {eta_hours:.2f} hours\", end=\"\")\n","\n","    print()  # New line after progress tracking\n","\n","    # Clear memory\n","    gc.collect()\n","    if USE_GPU:\n","        torch.cuda.empty_cache()\n","\n","    return results\n"],"metadata":{"id":"4U5AamWdgRim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_in_chunks(video_paths, output_dir, label, chunk_size=MAX_VIDEOS_PER_RUN):\n","    \"\"\"Process videos in chunks to avoid memory issues\"\"\"\n","    all_results = []\n","\n","    for i in range(0, len(video_paths), chunk_size):\n","        chunk = video_paths[i:i+chunk_size]\n","        print(f\"Processing chunk {i//chunk_size + 1}/{(len(video_paths) + chunk_size - 1)//chunk_size}\")\n","\n","        # Process chunk\n","        results = process_videos_in_parallel(chunk, output_dir, label)\n","        all_results.extend(results)\n","\n","        # Clear memory\n","        gc.collect()\n","        if USE_GPU:\n","            torch.cuda.empty_cache()\n","\n","        print(f\"Completed chunk with {len(results)} processed videos\")\n","\n","    return all_results"],"metadata":{"id":"SqZc72HMgRlS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_balanced_splits(real_paths, fake_paths, real_to_fake_ratio=1.0, test_size=0.2, val_size=0.1, seed=42):\n","    \"\"\"\n","    Create balanced train/val/test splits with a specific real-to-fake ratio\n","\n","    Args:\n","        real_paths: List of real video paths\n","        fake_paths: List of fake video paths\n","        real_to_fake_ratio: Target ratio of real:fake in each split (1.0 = equal)\n","        test_size: Proportion of data for test set\n","        val_size: Proportion of data for validation set\n","        seed: Random seed\n","\n","    Returns:\n","        Dictionary containing train/val/test splits for real and fake\n","    \"\"\"\n","    # First calculate how many samples we need in each split\n","    total_real = len(real_paths)\n","    total_fake = len(fake_paths)\n","\n","    # Split real videos\n","    real_train_val, real_test = train_test_split(real_paths, test_size=test_size, random_state=seed)\n","    real_train, real_val = train_test_split(\n","        real_train_val,\n","        test_size=val_size/(1-test_size),  # Adjust validation size\n","        random_state=seed\n","    )\n","\n","    # Split fake videos\n","    fake_train_val, fake_test = train_test_split(fake_paths, test_size=test_size, random_state=seed)\n","    fake_train, fake_val = train_test_split(\n","        fake_train_val,\n","        test_size=val_size/(1-test_size),  # Adjust validation size\n","        random_state=seed\n","    )\n","\n","    # Calculate target sizes for fake videos to match the ratio\n","    target_fake_train = int(len(real_train) / real_to_fake_ratio)\n","    target_fake_val = int(len(real_val) / real_to_fake_ratio)\n","    target_fake_test = int(len(real_test) / real_to_fake_ratio)\n","\n","    # Downsample fake videos in each split if needed\n","    if len(fake_train) > target_fake_train:\n","        fake_train = downsample_videos(fake_train, target_count=target_fake_train, seed=seed)\n","\n","    if len(fake_val) > target_fake_val:\n","        fake_val = downsample_videos(fake_val, target_count=target_fake_val, seed=seed)\n","\n","    if len(fake_test) > target_fake_test:\n","        fake_test = downsample_videos(fake_test, target_count=target_fake_test, seed=seed)\n","\n","    return {\n","        'real_train': real_train,\n","        'real_val': real_val,\n","        'real_test': real_test,\n","        'fake_train': fake_train,\n","        'fake_val': fake_val,\n","        'fake_test': fake_test\n","    }"],"metadata":{"id":"0e3oTiB8gRnz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import for as_completed\n","from concurrent.futures import as_completed\n","\n","def preprocess_dataset(balance_ratio=1.1, balance_splits=True):\n","    \"\"\"\n","    Preprocess all videos and create metadata files with balanced classes\n","\n","    Args:\n","        balance_ratio: Target ratio of real:fake videos (e.g., 1.1 means 10% more real than fake)\n","        balance_splits: Whether to balance train/val/test splits individually\n","    \"\"\"\n","    # Get all video files\n","    real_videos = get_video_files(REAL_DIRS)\n","    fake_videos = get_video_files(FAKE_DIRS)\n","\n","    print(f\"Found {len(real_videos)} real videos and {len(fake_videos)} fake videos\")\n","\n","    # Calculate target number of fake videos based on real videos\n","    target_fake_count = int(len(real_videos) / balance_ratio)\n","\n","    # Downsample fake videos if needed\n","    if len(fake_videos) > target_fake_count:\n","        print(f\"Downsampling fake videos from {len(fake_videos)} to {target_fake_count} for balance...\")\n","        fake_videos = downsample_videos(fake_videos, target_count=target_fake_count)\n","\n","    print(f\"After balancing: {len(real_videos)} real videos and {len(fake_videos)} fake videos\")\n","    print(f\"Class ratio (real:fake): {len(real_videos)/len(fake_videos):.2f}\")\n","\n","    # Process real videos in chunks\n","    print(\"Processing real videos...\")\n","    processed_real = process_in_chunks(real_videos, os.path.join(FACE_DIR, \"real\"), 'real')\n","\n","    # Process fake videos in chunks\n","    print(\"Processing fake videos...\")\n","    processed_fake = process_in_chunks(fake_videos, os.path.join(FACE_DIR, \"fake\"), 'fake')\n","\n","    print(f\"Processed {len(processed_real)} real videos and {len(processed_fake)} fake videos\")\n","\n","    # Handle empty dataset case\n","    if len(processed_real) == 0 or len(processed_fake) == 0:\n","        print(\"Warning: Not enough videos processed to create dataset splits\")\n","        return {\n","            'total': len(processed_real) + len(processed_fake),\n","            'real': len(processed_real),\n","            'fake': len(processed_fake)\n","        }\n","\n","    # Create splits with balancing\n","    if balance_splits:\n","        print(\"Creating balanced train/val/test splits...\")\n","        splits = create_balanced_splits(\n","            processed_real,\n","            processed_fake,\n","            real_to_fake_ratio=balance_ratio,\n","            test_size=0.2,\n","            val_size=0.1,\n","            seed=42\n","        )\n","\n","        real_train, real_val, real_test = splits['real_train'], splits['real_val'], splits['real_test']\n","        fake_train, fake_val, fake_test = splits['fake_train'], splits['fake_val'], splits['fake_test']\n","    else:\n","        # Original split code\n","        real_train, real_temp = train_test_split(processed_real, test_size=0.3, random_state=42)\n","        real_val, real_test = train_test_split(real_temp, test_size=0.5, random_state=42)\n","\n","        fake_train, fake_temp = train_test_split(processed_fake, test_size=0.3, random_state=42)\n","        fake_val, fake_test = train_test_split(fake_temp, test_size=0.5, random_state=42)\n","\n","    # Create metadata files more efficiently\n","    train_data = [{'path': path, 'label': 1} for path in real_train] + \\\n","                [{'path': path, 'label': 0} for path in fake_train]\n","\n","    val_data = [{'path': path, 'label': 1} for path in real_val] + \\\n","              [{'path': path, 'label': 0} for path in fake_val]\n","\n","    test_data = [{'path': path, 'label': 1} for path in real_test] + \\\n","               [{'path': path, 'label': 0} for path in fake_test]\n","\n","    # Save metadata\n","    pd.DataFrame(train_data).to_csv(os.path.join(METADATA_DIR, \"train_metadata.csv\"), index=False)\n","    pd.DataFrame(val_data).to_csv(os.path.join(METADATA_DIR, \"val_metadata.csv\"), index=False)\n","    pd.DataFrame(test_data).to_csv(os.path.join(METADATA_DIR, \"test_metadata.csv\"), index=False)\n","\n","    # Dataset statistics\n","    stats = {\n","        'train_real': len(real_train),\n","        'train_fake': len(fake_train),\n","        'val_real': len(real_val),\n","        'val_fake': len(fake_val),\n","        'test_real': len(real_test),\n","        'test_fake': len(fake_test),\n","        'total': len(processed_real) + len(processed_fake)\n","    }\n","\n","    print(\"\\nDataset Statistics:\")\n","    print(f\"Total videos: {stats['total']}\")\n","    print(f\"Training set: {stats['train_real']} real, {stats['train_fake']} fake\")\n","    print(f\"Validation set: {stats['val_real']} real, {stats['val_fake']} fake\")\n","    print(f\"Test set: {stats['test_real']} real, {stats['test_fake']} fake\")\n","\n","    # Print class balance statistics\n","    print(\"\\nClass Balance Statistics:\")\n","    print(f\"Total videos: {stats['total']}\")\n","    print(f\"Real videos: {len(processed_real)} ({len(processed_real)/stats['total']*100:.2f}%)\")\n","    print(f\"Fake videos: {len(processed_fake)} ({len(processed_fake)/stats['total']*100:.2f}%)\")\n","\n","    # Source distribution for real videos\n","    source_counts = {}\n","    for path in processed_real:\n","        source = path.split(os.sep)[-3] if \"face_clips\" in path else os.path.basename(os.path.dirname(path))\n","        source_counts[source] = source_counts.get(source, 0) + 1\n","\n","    print(\"\\nReal videos by source:\")\n","    for source, count in source_counts.items():\n","        print(f\"  {source}: {count} ({count/len(processed_real)*100:.2f}%)\")\n","\n","    return stats"],"metadata":{"id":"m_g8qdAdgRqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimize_colab_performance():\n","    \"\"\"Set Colab to high performance mode\"\"\"\n","    try:\n","        # Set OpenCV to use TBB for parallel processing\n","        cv2.setNumThreads(NUM_CORES)\n","\n","        # Only retain essential process memory\n","        os.environ['MALLOC_TRIM_THRESHOLD_'] = '65536'\n","\n","        # GPU-specific optimizations for T4\n","        if USE_GPU and torch.cuda.get_device_name(0).find('T4') != -1:\n","            # T4 optimizations\n","            torch.backends.cudnn.benchmark = True\n","            torch.backends.cudnn.deterministic = False\n","            # Set GPU memory optimization\n","            torch.cuda.set_per_process_memory_fraction(0.85)  # Use 85% of available GPU memory\n","            print(\"T4 GPU optimizations applied\")\n","\n","        print(\"Colab performance optimizations applied\")\n","    except Exception as e:\n","        print(f\"Error applying performance optimizations: {e}\")"],"metadata":{"id":"8gIG__Wdgk2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","if __name__ == \"__main__\":\n","    print(\"Optimizing Colab performance...\")\n","    optimize_colab_performance()\n","\n","    # Run preprocessing with balanced dataset\n","    # Perfect balance between real and fake\n","    stats = preprocess_dataset(balance_ratio=1.0, balance_splits=True)\n","\n","    # Clean up\n","    if USE_GPU:\n","        torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    print(\"Preprocessing complete!\")"],"metadata":{"id":"YZgnoBw7gks1"},"execution_count":null,"outputs":[]}]}